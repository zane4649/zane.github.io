<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Centos7 编译安装MariaDB-10.1.22]]></title>
    <url>%2F2017%2F09%2F26%2F%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[目的：使用源码包编译安装Mariadb 卸载系统安装的mariadb[root@localhost /]# rpm -qa | grep mariadb mariadb-libs-5.5.44-2.el7.centos.x86_64 [root@localhost /]# rpm -e mariadb-libs-5.5.44-2.el7.centos.x86_64 创建用户和组[root@localhost /]# groupadd mysql [root@localhost /]# useradd -g mysql mysql 创建文件目录[root@localhost /]# mkdir /mydata/data -pv [root@localhost /]# chown mysql.mysql data 安装依赖包[root@localhost /]# yum groupinstall -y Development Tools #环境开发工具包 [root@localhost /]# yum -y install gcc gcc-c++ make cmake ncurses ncurses-devel man ncurses libxml2 libxml2-devel openssl-devel bison bison-devel # cmake：由于从MySQL5.5版本开始弃用了常规的configure编译方法，所以需要cmake编译器，用于设置mysql的编译参数。（如：安装目录，数据存放目录，字符编码，排序规则等） # boost：从MySQL5.7.5开始Boost库是必需的，mysql源码中用到了C++的Boost库，要求必需安装Boost1.59.0或以上版本。 # GCC：这是Linux下的C语言编译工具，MySQL源码编译完全由C和C++编写，要求必需安装GCC。 # bison：Linux下C/C++语法分析器 # ncurses：字符终端处理库。 ......... 编译准备[root@localhost /]# tar -xvf mariadb-10.1.22.tar.gz -C /usr/local [root@localhost /]# cd /usr/local/ [root@localhost /]# ln -sv mariadb-10.1.22 mysql #做软链接 [root@localhost /]# chown -R root:mysql ./* #修改配置文件的属组 开始编译[root@localhost mysql]# cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql #Mysql安装的根目录 -DMYSQL_DATADIR=/mydata/data #Mysql数据库文件存放路径 -DWITH_INNOBASE_STORAGbE_ENGINE=1 #添加InoooDB引擎 -DWITH_ARCHIVE_STORAGE_ENGINE=1 #添加ARCHIVE引擎 -DWITH_BLACKHOLE_STORAGE_ENGINE=1 #添加BLACKHOLE引擎 -DWITH_SSL=system -DWITH_ZLIB=system #可以用systemd控制服务 -DWITH_LIBWRAP=0 # -DMYSQL_UNIX_ADDR=/tmp/mysql.sock #指定sock文件的位置 -DEFAULT_CHARSET=utf8 #Mysql默认字符集为utf-8 -DDEFAULT_COLLATION=utf8_general_ci #支持默认字符集校对规则 [root@localhost mysql]# make &amp;&amp; make install #编译并安装 初始化数据库四个数据库，information_schema ，performance_schema，test ，mysql源数据库（用户，权限，字段，字段名。。。。）, 自行脚本生成，脚本在mysql/scripts 执行一下就可以安装了 [root@localhost mysql]# scripts/mysql_install_db --user=mysql --datadir=/mysql/data [root@localhost mysql]# scripts/mysql_install_db --help [root@localhost mysql]# ls /mydata/data/ aria_log.00000001 ib_logfile0 multi-master.info mysql-bin.index aria_log_control ib_logfile1 mysql performance_schema ibdata1 localhost.localdomain.pid mysql-bin.000001 test 创建配置文件[root@localhost mysql]# mkdir /etc/mysql [root@localhost mysql]# cp support-files/my-large.cnf /etc/mysql/my.cnf [root@localhost mysql]# vim /etc/mysql/my.cnf datadir = /mydata/data #数据文件的路径 innodb_file_per_table = on #修改InnoDB为独立表空间模式 skip_name_resolve = on #跳过名称反解，将客户端连接IP反解成主机名，然后做权限检查 创建服务脚本[root@localhost mysql]# cp support-files/mysql.server /etc/rc.d/init.d/mysqld [root@localhost mysql]# chkconfig --add mysqld #设置开机启动 [root@localhost mysql]# chkconfig --list mysqld [root@localhost mysql]# service mysqld start [root@localhost mysql]# ss -tnlp #查看3306是否被mysql监听 LISTEN 0 80 :::3306 :::* users:((&quot;mysqld&quot;,pid=5358,fd=21)) 设置环境变量[root@localhost mysql]# vim /etc/profile.d/mysqld.sh MYSQL_HOME=/usr/local/mysql export PATH=$MYSQL_HOME/bin:$PATH [root@localhost mysql]# source /etc/profile.d/mysqld.sh #加载环境变量 连接Mysql[root@localhost mysql]# mysql Welcome to the MariaDB monitor. Commands end with ; or \g. Your MariaDB connection id is 4 Server version: 10.1.22-MariaDB Source distribution Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. MariaDB [(none)]&gt; 遇到的问题启动时报错： /usr/local/mysql//libexec/mysqld: Can&apos;t create/write to file &apos;/var/log/mariadb&apos; (Errcode: 13) 120516 15:23:19 [ERROR] Can&apos;t start server: can&apos;t create PID file: Permission denied 解决方法： chown mysql.mysql /var/log/mariadb/ -R]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>编译安装Mariadb-10.1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 HAproxy实现反向代理和资源动静分离]]></title>
    <url>%2F2017%2F09%2F26%2Fhaproxy%2F</url>
    <content type="text"><![CDATA[HAProxy 是一款提供高可用性、负载均衡以及基于TCP（第四层）和HTTP（第七层）应用的代理软件， 支持虚拟主机，它是免费、快速并且可靠的一种解决方案。 HAProxy特别适用于那些负载特大的web站点，这些站点通常又需要会话保持或七层处理。HAProxy运行在时下的硬件上，完全可以支持数以万计的 并发连接。并且它的运行模式使得它可以很简单安全的整合进您当前的架构中， 同时可以保护你的web服务器不被暴露到网络上。HAProxy 实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。多进程或多线程模型受内存限制 、系统调度器限制以及无处不在的锁限制，很少能处理数千并发连接。 事件驱动模型因为在有更好的资源和时间管理的用户端(User-Space) 实现所有这些任务，所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。这就是为什么他们必须进行优化以 使每个CPU时间片(Cycle)做更多的工作。 总结：HAproxy只是Http协议的反向代理，不提供内存，但额外支持对TCP层基于Http通信的应用做负载均衡 架构图 HAproxy工作模式： 1、Http反向代理 2、TCP连接负载均衡 HAproxy的配置文件格式： global: 设定全局配置参数 proxy：定义”defaults” “listen” “frontend” “backend” 三台主机： HAProxy:192.168.1.51 Web1:192.168.1.52 Web2:192.168.1.53 目的：利用HAproxy实现反向代理和资源动静分离 利用HAproxy实现反向代理Web1主机安装httpd [root@web1 ~]# yum -y install httpd [root@web1 ~]# echo web1 &gt; /var/www/html/index.html [root@web1 ~]# systemctl start httpd [root@web1 ~]# firewall-cmd --permanent --add-service=http [root@web1 ~]# firewall-cmd --reload [root@web1 ~]# curl http://192.168.1.52 web1 Web2主机配置同上 Proxy主机安装HAproxy [root@proxy ~]# yum -y install haproxy.x86_64 [root@proxy ~]# vim /etc/haproxy/haproxy.cfg chroot /var/lib/haproxy #工作目录 pidfile /var/run/haproxy.pid #pid文件 maxconn 4000 #最大连接数 user haproxy #进程启动以哪个用户来运行 group haproxy daemon #启动为守护进程，不加daemon将会运行在前台 frontend main *:80 #监听的端口 efault_backend webserver backend webserver balance roundrobin #使用roundrobin算法（rr轮询） #用法:server &lt;name&gt; &lt;addr&gt;[:port] [param] server web1 192.168.1.52:80 check weight=1 #check做健康监测，默认2秒一次,weight定义权重 server web2 192.168.1.53:80 check weight=1 [root@proxy ~]# systemctl start haproxy.service [root@proxy ~]# curl http://192.168.1.51 web1 [root@proxy ~]# curl http://192.168.1.51 web2 [root@proxy ~]# firewall-cmd --permanent --add-port=80/tcp [root@proxy ~]# firewall-cmd --reload 基于浏览器cookie实现session stickebackend webserver balance roundrobin cookie SERVERID insert nocahe #设置cookie,insert插入，nocache不允许缓存，indirect间接方式让cookie生效 server web1 192.168.1.52:80 check weight=1 cookie websrv1 #绑定cookie并指定cookie名称 server web2 192.168.1.53:80 check weight=1 cookie websrv2 要点： （1）每个server有自己唯一的cookie标识 （2）在backend中定义为用户请求调度完成操作其cookie Haproxy的几种调度方法动态：（权重可以动态调整） 静态：（调整权重不会立即生效，需要重启服务生效） 1、Roundrobin(rr轮询）:根据权重的比率在服务器之间公平调度，每个后端服务器最多可以承载4128个并发连接，属于动态 2、Leastcinn: 根据后端服务器的负载数量进行调度，仅适用于长连接回话，属于动态 3、Static（rr轮询）:和roundrobin类似，但是不支持实时修改权重，后端服务器并发连接理论没有上限，属于静态， 4、Source：将连接请求的源地址进行Hash计算，并由后端服务器的权重总和相除后转发至匹配的服务器，是否静态或动态，取决于Hash-type 5、URI: 对uri左半部分或对整个uri进行Hash计算,并由服务器的总权重相除后派至匹配后的某个服务器，特别适用于代理缓存服务器应用场景 6、Hdr&lt;name&gt;: 根据请求报文中指定的header,（如：user_agent,referer,hostname）进行调度，把指定的header的值做hash计算,Hash_type 7、URL_param: 根据url中指定参数的值进行调度，把值做Hash计算，并除以总权重，Hash_type ##只要用到Hash算法，就会根据Hash_type的值来确定是动态还是静态 Hash-type： 1、map-hash(取模法)，静态 2、consistent(一致性哈希)，动态 配置stats管理界面[root@proxy ~]# vim /etc/haproxy/haproxy.cfg frontend main bind *:80 default_backend webserver listen stat bind 192.168.1.51:9000 #管理页面的端口 stats enable #开启stats页面 stats hide-version #隐藏版本号 stats uri /admin #访问stats的url stats realm haproxy\ admin\ area #需要认证时，认证框里面的提示信息 stats auth admin:admin #认证的账号和密码（user:password） stats refresh 5s #打开页面后每秒刷新间隔 stats admin if TRUE #是否开启管理功能 然后浏览器输入 192.168.1.51:9000/admin 根据提示输入用户和密码 密码验证： 管理界面： 利用Haproxy实现动静分离[root@proxy ~]# vim /etc/haproxy/haproxy.cfg frontend main bind *:80 default_backend dynamic #默认backend为dynamic acl url_static path_end -i .php #访问控制列表，匹配结尾.php资源 use_backend static if url_static #如果结尾是.php，则backend为static backend static balance roundrobin #这里使用roundrobin算法 server dynamic 192.168.1.53:80 check backend dynamic balance uri#这里使用uri算法 server static 192.168.1.52:80 check 客户端资源验证静态资源动态资源]]></content>
      <categories>
        <category>HAproxy</category>
      </categories>
      <tags>
        <tag>HAproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 HA集群之Corosync+Pacemaker+NFS+Http+Ansible]]></title>
    <url>%2F2017%2F09%2F26%2Fpacemaker%2Bcorosync%2F</url>
    <content type="text"><![CDATA[Corosync是OpenAIS发展到Wilson版本后衍生出来的开放性集群引擎工程。可以说Corosync是OpenAIS工程的一部分。OpenAIS从openais0.90开始独立成两部分，一个是Corosync；另一个是AIS标准接口Wilson。Corosync包含OpenAIS的核心框架用来对Wilson的标准接口的使用、管理。它为商用的或开源性的集群提供集群执行框架。Corosync执行高可用应用程序的通信组系统，它有以下特征：（1）一个封闭的程序组（A closed process group communication model）通信模式，这个模式提供一种虚拟的同步方式来保证能够复制服务器的状态。（2）一个简单可用性管理组件（A simple availability manager），这个管理组件可以重新启动应用程序的进程当它失败后。（3）一个配置和内存数据的统计（A configuration and statistics in-memory database），内存数据能够被设置，回复，接受通知的更改信息。（4）一个定额的系统（A quorum system）,定额完成或者丢失时通知应用程序。总结：Corosync是整合底层节点服务器，提供API个pacemaker的一个中间层 Pacemaker，是一个群集资源管理器。它实现最大可用性群集服务（亦称资源管理）的节点和资源级故障检测和恢复使用您的首选集群基础设施（OpenAIS的或Heaerbeat）提供的消息和成员能力。它可以做乎任何规模的集群，并配备了一个强大的依赖模型，使管理员能够准确地表达群集资源之间的关系（包括顺序和位置）。几乎任何可以编写脚本，可以管理作为心脏起搏器集群的一部分。pacemaker支持超过16个节点的控制 目的：利用Corosyn+Packemaker+NFS+Http实现Web高可用集群 工作原理图（图片来源于网络）： 架构图： 准备四台机器和一个虚拟IP： 虚拟IP:192.168.1.60 node1:192.168.1.51 node1.com node2:192.168.1.52 node2.com nfs :192.168.1.53 ansible:192.168.1.54 集群准备工作 两节点要时间同步，ssh访问互信，hosts名称解析一致,关闭selinux 1.修改Hosts [root@node1 ~]# vim /etc/hosts 192.168.1.51 node1.com node1 192.168.1.52 node2.com node2 [root@node1 ~]# uname -n node1.com 其他节点和上面配置一样 2.配置ansible主机 [root@ansible ~]# yum -y install ansible （需要配置eple源） [root@ansible ~]# cp hosts{,.bak} ##复制一份备用 [root@ansible ~]# vim /etc/ansible/hosts [haservers] ##定义一个ansible集群组 192.168.1.51 192.168.1.52 3.建立ssh公钥认证 [root@ansible ~]# ssh-keygent -t rsa -P&apos;&apos; ##生成公钥（一路回车就行了） [root@ansible ~]# ssh-copy-id -i .ssh/id_rsa.pub root@192.168.1.51 ##复制公钥到node1.com [root@ansible ~]# ssh-copy-id -i .ssh/id_rsa.pub root@192.168.1.52 ##复制到node2.com 4.测试一下： [root@ansible ~]# ssh node1 The authenticity of host &apos;node1 (192.168.1.51)&apos; can&apos;t be established. ECDSA key fingerprint is 85:a3:4b:1d:ab:2f:41:30:df:70:39:76:f4:08:be:02. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;node1&apos; (ECDSA) to the list of known hosts. Last login: Tue Feb 28 22:07:37 2017 [root@node1 ~]# ##没有输密码表示成功 node2节点也试一下 [root@ansible ~]# ansible all -m ping ##使用ansible测试一下 192.168.1.51 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.52 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 同步时间： [root@ansible ~]# ansible all -m yum -a &quot;name=ntpdate state=present&quot; ##各节点都装上时间服务器 [root@ansible ~]# ansible all -m shell -a &quot;ntpdate cn.pool.ntp.org&quot; ##向国家时间服务器同步 安装corosync,pacemaker,crmsh 1.首先安装corosync和pacemaker 由于corosync是pacemake的依赖包，所有安装完pacemaker后corosync自动被安装上 使用ansible安装: [root@ansible ~]# ansible all -m yum -a &quot;name=pacemaker state=present&quot; 在2个node节点确认： [root@node1~]# rpm -qa pacemaker pacemaker-1.1.13-10.el7.x86_64 [root@node1~]# rpm -qa corosync corosync-2.3.4-7.el7.x86_64 2.安装crmsh crmsh在yum仓库和epel源都没有需要去另外下载,下载后拷贝到ansible主机/crmsh http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-7/noarch/ [root@ansible ~]#ls /crmsh asciidoc-8.6.9-32.2.noarch.rpm crmsh-scripts-3.0.0-1.2.noarch.rpm asciidoc-examples-8.6.9-32.2.noarch.rpm crmsh-test-3.0.0-1.2.noarch.rpm crmsh-3.0.0-1.2.noarch.rpm python-parallax-1.0.1-28.2.noarch.rpm [root@ansible ~]# ansible all -m shell -a &apos;mkdir /root/crmsh&apos; [root@ansible ~]# ansible all -m copy -a &quot;src=/root/crmsh/ dest=/root/crmsh/&quot; [root@ansible ~]# ansible all -m shell -a &apos;yum -y install /root/crmsh/*.rpm&apos; 两个节点确认一下: [root@node1 ~]#crm crm(live)# ##表示crmsh安装成功 配置corosync和pacemaker并启动服务 corosync配置文件修改，ansible主机上修改并部署 [root@ansible ~]# yum -y install pacemaker [root@ansible ~]# cd /etc/corosync [root@ansible ~]# cp corosync.conf.example corosync.conf [root@ansible ~]# vim corosync.conf # Please read the corosync.conf.5 manual page totem { ##集群信息 version: 2 ##版本 crypto_cipher: aes128 ##对称节点采用aes128加密方式，单项采用sha1加密 crypto_hash: sha1 interface { ringnumber: 0 ##心跳信息传递的环号码，如果有多个接口传递心跳信息，则定义多个，每个环号不同 bindnetaddr: 192.168.1.0 ##心跳信息接口的网络地址 mcastaddr: 239.255.1.1 ##心跳信息传递时使用的组播地址 mcastport: 5405 ##组播端口 ttl: 1 ##防止心跳信息环路，限定心跳信息传递的ttl值，一般是集群节点个数减去1 } } logging { ##定义日志记录方式 fileline: off to_stderr: no to_logfile: yes ##是否记录日志 logfile: /var/log/cluster/corosync.log ##日志文件位置 to_syslog: no ##是否记录到系统日志 debug: off ##关闭调试模式 timestamp: on ##时间戳标签 logger_subsys { ##记录投票子系统的日志信息 subsys: QUORUM debug: off } } quorum { provider: corosync_votequorum ##投票系统使用corosync自带 } nodelist { ##自定义的节点 node { ring0_addr:node1.com ##环0上的地址，如果此前定义了多个接口用于传递心跳信息， 则次处定义多个环上的不同地址，如ring1_addr:.... nodeid:1 } node { ring0_addr:node2.com nodeid:2 } } 创建认证文件 [root@ansible ~]# corosync-keygen -l ##由于corosync配置文件中定义了通信采用的加密方式进行，因此要生成秘钥文件 使用ansible将配置文件及认证文件全部拷贝至节点服务器，注意authkey的权限（400或600） [root@ansible ~]# ansible all -m copy -a &quot;src=/etc/corosync/authkey mode=400 dest =/etc/corosync/authkey&quot;##复制认证文件到各节点 [root@ansible ~]# ansible all -m copy -a &quot;src=/etc/corosync/corosync.confdest=/etc/corosync/corosync.conf&quot;##复制corosync配置文件到各节点 进入节点主机进行验证: [root@node1~]# ll /etc/corosync/ total 20 -r--------. 1 root root 128 Feb 23 02:47 authkey -rw-r--r--. 1 root root 2999 Feb 27 22:32 corosync.conf -rw-r--r--. 1 root root 2881 Nov 20 2015 corosync.conf.example -rw-r--r--. 1 root root 767 Nov 20 2015 corosync.conf.example.udpu -rw-r--r--. 1 root root 3278 Nov 20 2015 corosync.xml.example drwxr-xr-x. 2 root root 6 Nov 20 2015 uidgid.d 开启corosync和pacemaker服务： [root@ansible ~]# ansible all -m service -a &quot;name=corosync state=started&quot; ##启动各节点的corosync服务 [root@ansible ~]# ansible all -m service -a &quot;name=pacemaker state=started&quot; ##启动各节点的pacemaker服务 各节点查看服务状态： [root@node1~]# crm status Last updated: Tue Feb 28 23:32:05 2017 Last change: Tue Feb 28 22:03:05 2017 by root via cibadmin on node1.com Stack: corosync Current DC: node1.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum 2 nodes and 3 resources configured Online: [ node1.com node2.com ] ##node1和node同时在线表示成功 配置NFS服务安装配置nfs服务 [root@nfs server ~]# yum -y install nfs-utils ##安装nfs [root@nfs server ~]# systemctl enable nfs.service ##开机启动nfs [root@nfs server ~]# systemctl start nfs.service ##启动nfs [root@nfs server ~]# firewall-cmd --permanent --add-service=nfs ##防火墙放行nfs [root@nfs server ~]# firewall-cmd --permanent --add-service=rpc-bind [root@nfs server ~]# firewall-cmd --permanent --add-service=mountd [root@nfs server ~]# firewall-cmd --reload [root@nfs server ~]# mkdir /web/storage -pv [root@nfs server ~]# vim /etc/exportfs /web/storage 192.168.1.0/24(rw) [root@nfs server ~]#exportfs -rv ##(-r表示重读配置，-v显示共享情况) [root@nfs server ~]# systemctl restart nfs.service 使用ansible挂载nfs [root@ansible ~]# ansible all -m yum -a &quot;name=nfs-utils state=present&quot; [root@ansible ~]# ansible all -m shell -a &apos;mount -t nfs 192.168.1.53:/web/storage /var/www/html&apos; 到node节点上查看是否挂载 [root@node1 ~]# df -h Filesystem Size Used Avail Use% Mounted on 192.168.1.53:/web/storage 10G 4.1G 6.0G 41% /var/www/html ansible卸载nfs（这里先卸载，因为后面集群会自动挂载） [root@ansible ~]# ansible all -m shell -a &apos;umount /var/www/html’ crmsh配置集群 [root@node1~]# crm(live)cd configure crm(live)configure# property no-quorum-policy=stop ##关闭法定票数不足 crm(live)configure# property stonith-enabled=false ##由于没有stonith设备，所以先将stonith关闭 crm(live)configure# rsc_defaults resource-stickiness=100 ##设定资源粘性值为100 crm(live)configure#primitive webip ocf:heartbeat:IPaddr params ip=192.168.1.60 ##定义虚拟IP 定义完虚拟IP可以验证一下，看各节点是否都配置成功 [root@node1~]# ip addr 2: eno16777736: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:9b:93:76 brd ff:ff:ff:ff:ff:ff inet 192.168.1.51/24 brd 192.168.1.255 scope global eno16777736 valid_lft forever preferred_lft forever inet 192.168.1.60/24 brd 192.168.1.255 scope global secondary eno16777736 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9b:9376/64 scope link valid_lft forever preferred_lft forever crm(live)configure# primitive webserver systemd:httpd op start timeout=100 op stop timeout=100 op monitor interval=60 timeout=100 ##定义使用的服务 crm(live)configure# primitive webstore ocf:heartbeat:Filesystem params directory=&quot;/var/www/html&quot; fstype=nfs op start timeout=60s op stop timeout=60s op monitor interval=60 timeout=100 ##定义使用的存储设备 crm(live)configure# group webservice webip webstore webserver ##定义集群组，要按照资源顺序来 crm(live)configure# verify ##校验 crm(live)configure# commit ##提交配置确认并生效 配置完成后查看是这样，表示成功 crm(live)# status Last updated: Wed Mar 1 01:27:39 2017 Last change: Tue Feb 28 22:03:05 2017 by root via cibadmin on node1.com Stack: corosync Current DC: node1.com (version 1.1.13-10.el7-44eb2dd) - partition with quorum 2 nodes and 3 resources configured Online: [ node1.com node2.com ] Full list of resources: Resource Group: webservice webip (ocf::heartbeat:IPaddr): Started node1.com webstore (ocf::heartbeat:Filesystem): Started node1.com webserver (systemd:httpd): Started node1.com 可以用下面几种资源约束的方法： crm(live)configure# location webstore_perfer webstore inf: node1.com ##使用location定义webstore对节点1的倾向性为正无穷 crm(live)configure# colocation webserver_with_webip inf:webserver webip ##定义两个资源的粘性，必须在一起 crm(live)configure# order webip_bef_webstore_bef_webserver mandatory: webip webstore webse ##强制资源启动顺序 crm(live)resource# migrate webip node1.com ##使用migrate进行资源转移 开始测试集群：[root@node1~]# echo The page is NFS Server &gt;&gt; /var/www/html/index.html [root@node1~]# curl http://192.168.1.60 The page is NFS Server [root@node1~]# crm node standby ##停用当前节点 [root@node1~]# crm status Node node1.com: standby Online: [ node2.com ] Full list of resources: Resource Group: webservice webip (ocf::heartbeat:IPaddr): Started node2.com webstore (ocf::heartbeat:Filesystem): Started node2.com webserver (systemd:httpd): Started node2.com 此时节点已经转移到了node2 OK，到此结束，全部成功了 遇到的问题：1、 2个节点双方都显示自己这一方在线，另一方不在线 解决方法：彻底关闭selinux 2、[root@node1 ~]#crm status ERROR: status: crm_mon (rc=107): Connection to cluster failed: Transport endpoint is not connected 解决方法：systemctl start packemaker.service ##ok!]]></content>
      <categories>
        <category>Cluster</category>
      </categories>
      <tags>
        <tag>HA之Corosync+Pacemaker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 Nginx利用“反向代理”实现“动静分离”]]></title>
    <url>%2F2017%2F09%2F26%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Nginx是一款免费的，开源的，高性能的HTTP服务软件，它不仅能够支持反向代理服务器，而且也可以当作IMAP/POP3代理服务。nginx不采用每客户机一线程的设计模型，而是充分使用异步逻辑，削减了上下文调度开销，所以并发服务能力更强。整体采用模块化设计，有丰富的模块库和第三方模块库，配置灵活。在Linux操作系统下，nginx使用epoll事件模型，得益于此，nginx在Linux操作系统下效率相当高。同时Nginx在OpenBSD或FreeBSD操作系统上采用类似于epoll的高效事件模型kqueue。而且它稳定，配置丰富，设置简单，支持热部署，而且占用系统资源少，10000个keep-alive模式下connection仅需要2.5MB内存！！！（俄罗斯的产品果然是简单粗暴） Nginx工作模式：Master/worker 一个主进程master生成一个或者多个work线程 master:负责加载和分析配置文件 work:处理并响应用户请求 特性： 异步，非阻塞，事件驱动 文件IO: sendfile ，mmap 核心模块： main: 主要用户配置错误日志，进程管理，权限控制 event: 配置epoll,kqueue,select,poll rewrite模块： 把用户请求的URL基于regex做检查，匹配时将替换为replacement指定的字符串 如果replacement是以http：//开头，则替换结果会直接重定向返回客户端 在同一个localtion中存在多个rewrite规则会自上而下逐个检查，可以使用flag控制此循环功能 last：提前结束本轮循环，进入下一轮，continuebreak：终止了，不再循环redirect：重定向，临时重定向，302permanent：永久重定向，301，重写后生成的新url给客户端，由客户端对新url进行请求 例如： localtion / { rewrite (.）.txt$ $1.html; }请求的txt文件都变成html，重写，变成了一个新的url，然后被匹配 再加上： location ~.html\$ { rewrite (.*).html $1.txt; }后，html又转化为txt了，就形成一个死循环，所以要在两个后面加上条件，break。 然后再加上个redirect，临时重定向 location / { rewrite (.*).txt$ $1.html redirect; } upstream 模块： 负载均衡，必须定义在http段落 实验目的：利用Nginx反向代理，将用户请求的动态资源反代到Web1,静态资源反代到Web2 架构图： 虚拟机实验环境1.关闭selinux和清空iptables 2.所有主机同步时间 3.Nginx反向代理服务器内网网卡选择VMNET8虚拟网络连接 4.Web1部署LAMP平台，Web2部署httpd 网络配置 客户端IP：192.168.1.158 Nginx反向代理服务器IP： 外网：192.168.1.51 内网：192.168.42.130 后端Web服务器IP： Web1：192.168.42.128 Web2：192.168.42.129 服务安装Nginx服务器安装nginx软件包： [root@nginx /]# yum -y install nginx ##需要配置好epel源 Web1服务器配置搭建LAMP平台： [root@Web1 /]# yum -y install httpd mariadb-server php php-mysql php-gd libjpeg* php-ldap php-odbc php-pear php-xml php-xmlrpc php-mbstring php-bcmath php-mhash 新建测试页 vim /var/www/html/index.php &lt;?php phpinfo(); ?&gt; [root@Web1 /]# curl http://192.168.42.128 能看到php测试页表示ok Web2服务器配置httpd服务 [root@Web2 /]# yum -y install httpd 安装完后新建测试页 curl http://192.168.42.129 能看到表示ok nginx配置反向代理[root@nginx nginx]# cp nginx.conf{,.bak} ##先做备份 主要配置： proxy_set_header NWC_TEST $remote_addr; ##定义日志首部 location / { root /usr/share/nginx/html; index index.html index.htm; proxy_pass http://192.168.42.129; } location ~ \.php$ { proxy_pass http://192.168.42.128; index index.php index.html index.htm; } 测试http://192.168.1.51/index.php|index.html 能看到.php页面和.html页面的内容和WEB1,WEB2相同表明OK 设置RS的请求报文首部在nginx主机上添加 proxy_set_header NWC_TEST $remote_addr; 利用proxy_set_header指令，设置nginx反代用户请求到后端主机上时，为请求报文添加一个首部， 首部名称为NWC_TEST,首部对应的值为$remote-addr对应的值，也就是真是客户端IP地址 修改完nginx服务器，再到后端主机修改日志格式 LogFormat &quot;%{NWC_TEST}i %h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%{Referer}i\&quot; \&quot;%{User-Agent}i\&quot;&quot; combined]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 Mysql双主模型]]></title>
    <url>%2F2017%2F09%2F26%2Fmysql%E5%8F%8C%E4%B8%BB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Mysql双主模型可以在一定程度上保证主库的高可用,在一台主库down掉之后,可以在极短的时间内切换到另一台主库上（尽可能减少主库宕机对业务造成的影响），减少了主从同步给线上主库带来的压力； 目的：利用双主模型来提高系统的可用性 模型图（来源于网络）： 准备工作1、为了不影响实验结果，建议关闭Selinux和Iptables2、两台服务器时间要同步3、两台服务器都要启动中继日志，二进制日志4、主服务器1: 192.168.1.60 主服务器2：192.168.1.61 安装Mariadb[root@mysql-masster1 /]# yum -y install mariadb-server mariadb第二台服务器一样直接安装 修改主配置文件Master1配置： [root@mysql-masster1 /]# vim /etc/my.cnf [mysqld] server_id=1 ##设定全局唯一的server_id log_bin=master-log ##启用二进制日志，指明二进制日志的存放路径及名称 relay_log=relay-log ##启用中继日志，指明中继日志的存放路径及名称 auto_increment_offset=1 ##避免自增长字段的冲突值，设置本机的自增长字段从1开始，每次增长2 auto_increment_increment=2 skip-name-resolve=ON ##禁止名称解析 innodb-file-per-table=ON ##innodb存储引擎每表一个表空间 Master2配置： [root@mysql-masster2 /]# vim /etc/my.cnf [mysqld] server_id=2 ##设定全局唯一的server_id log-bin=master-log ##启用二进制日志，指明二进制日志的存放路径及名称 relay-log=relay-log ##启用中继日志，指明中继日志的存放路径及名称 auto_increment_offset=2 ##避免自增长字段的冲突值，设置本机的自增长字段从1开始，每次增长2 auto_increment_increment=2 skip-name-resolve=ON ##禁止名称解析 innodb-file-per-table=ON ##innodb存储引擎每表一个表空间 **Master1和Master2配置完成后，重新启动mariadb服务** 登录MySQL用户授权Master1配置： [root@mysql-masster1 /]#mysql -uroot -p MariaDB [mysql]&gt; grant replication client,replication slave on *.* to &apos;repluser&apos;@&apos;192.168.1.61&apos; identified by &apos;redhat&apos;; MariaDB [mysql]&gt; flush privileges; MariaDB [mysql]&gt; show master logs; ##查看二进制日志列表和当前正在使用的二进制日志的文件位置 MariaDB [mysql]&gt; show master status; +-------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +-------------------+----------+--------------+------------------+ | master-log.000001 | 245| | | | +-------------------+----------+--------------+------------------+ MariaDB [mysql]&gt; ls /var/lib/mysql/ 查询二进制文件是否存在 Master2配置： [root@mysql-masster2 /]#mysql -uroot -p MariaDB [mysql]&gt; grant replication client,replication slave on *.* to &apos;repluser&apos;@&apos;192.168.1.60&apos; identified by &apos;redhat&apos;; MariaDB [mysql]&gt; flush privileges; MariaDB [mysql]&gt; show master logs; MariaDB [mysql]&gt; show master status; +-------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +-------------------+----------+--------------+------------------+ | master-log.000003 | 2109 | | | +-------------------+----------+--------------+------------------+ MariaDB [mysql]&gt; ls /var/lib/mysql/ 定义2个节点 在两个节点上定义复制时的属性，启动复制线程 Master1的配置： MariaDB [mysql]&gt; change master to master_host=&apos;192.168.1.61&apos;, ##指定的Master主机 master_user=&apos;repluser&apos;, ##以哪个用户的身份练上去 master_password=&apos;redhat&apos;, ##连接用户的密码 master_log_file=&apos;master-log.000003&apos;, ##主节点的二进制日志文件 master_log_pos=2109; ##主节点的二进制日志所在位置 Master2的配置： change master to master_host=&apos;192.168.1.60&apos;, master_user=&apos;repluser&apos;, master_password=&apos;redhat&apos;, master_log_file=&apos;master-log.000001&apos;, master_log_pos=245; 启动复制线程 [root@mysql-masster2 /]#mysql -uroot -p MariaDB [mysql]&gt;start slave; ##启动复制线程 MariaDB [mysql]&gt;show salve status; Slave_IO_Running:YES ##IO线程和SQL线程 Slave_SQL_Running:YES 两个线程都是YES，表明成功了，如果IO线程状态是Connecting 请退出MySQL，这个地方稍微有点延迟,重新启动一下mariadb服务，再连进去查看线程状态 双主服务器互相验证在Master1上创建一个testdb库，看Master2能否看见。Master2在testdb库中创建t1表，Master1能看见表明成功Master1在t1插入数据，Master能看见表示成功]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql双主模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 Mysql主从同步]]></title>
    <url>%2F2017%2F09%2F26%2FMysql-%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[MySQL主从同步：MySQL的主从复制广泛用于数据库备份、故障转移、数据分析等场合主从同步原理*：通过设置在Master MySQL上的binlog(使其处于打开状态)，Slave MySQL上通过一个I/O线程从Master MySQL上读取binlog，再传输到Slave MySQL的中继日志中，然后Slave MySQL的SQL线程从中继日志中读取中继日志，应用到Slave MySQL的数据库中。这样实现了主从数据同步功能。 目的：主从复制能够有效的缓解数据库读写的压力 模型图（图片来源于网络）： 环境的准备1.Mysql主备之前版本必须一致 2.为了不影响实验结果建议关闭&apos;Selinux&apos; 主服务器：mysql-master: 192.168.1.60 从服务器：mysql_slave: 192.168.1.61 安装Mariadb [root@mysql-master /]yum -y install mariadb mariadb-server [root@mysql-master /]systemctl enable mariadb.service ##开机启动mariadb服务 [root@mysql-master /]systemctl start mariadb.service ##启动mariadb服务 [root@mysql-master /]mysql_secure_installation ##初始化Mysql 从服务器同上操作 修改配置文件主服务器的配置文件（/etc/my.cnf）,开启日志功能，设置server_id 保证唯一 [root@mysql-master tmp]# vim /etc/my.cnf 加入以下两行内容 [mysqld] server_id = 200 ##server_id 同步复制时标识该语句最初是从哪个server写入的 log-bin=mysql-bin ##开启二进制功能 保存退出，重启服务器 数据库授权[root@mysql-master /]# mysqldump -uroot -p MariaDB [mysql]&gt; grant replication slave,reload,super on *.* to &apos;slave&apos;@&apos;192.168.1.60&apos; identified by &apos;slavepass&apos; MariaDB [mysql]&gt;flush privileges ##重读授权表信息 注释： ‘replication slave’拥有此权限可以查看从服务器，从主服务器读取二进制日志 ‘reload’权限，才可以执行flush [tables | logs | privileges] ‘super’这个权限允许用户终止任何查询；修改全局变量的SET语句；使用CHANGE MASTER，PURGE MASTER LOGS。 MariaDB [mysql]&gt;show master status; ##查看主服务的状态 +----------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +----------------------+----------+--------------+------------------+ | mysql-bin.000002 | 245| | | +----------------------+----------+--------------+------------------+ 主从备份为保证主DB server和从DB server的数据一致,这里采用主备份,从还原来实现初始数据一致 MariaDB [mysql]&gt;flush tables with read lock; ##加临时锁，表只读 [root@mysql-master /]# mysqldump -uroot -p --all- databases &gt; /home/backup.sql ##复制所有 数据库到/home/ [root@mysql-master /]# unlock tables; J##解锁 [root@mysql-master /]# scp /home/backup.sql ‘root’@‘192.168.1.61’：/home/ ##将备份数据发送到从服务器，用于恢复 配置Slave[root@mysql-slave /]# vim /etc/my.cnf server_id=201 ##设置server_id Slave还原备份数据[root@mysql-slave /]#systemctl restart mariadb.service [root@mysql-slave /]#mysql -uroot -p &lt; /home/backup.sql 连接Master服务器登陆从数据库,添加相关参数(主服务器的ip/端口/同步用户/密码/position号/读取哪个日志文件) [root@mysql-slave /]#mysql -uroot -p MariaDB [mysql]&gt;change master to master_host=&apos;192.168.1.60&apos;,master_user=&apos;slave&apos;,master_password=&apos;slavepass&apos;, master_log_file=&apos;mysql-bin.000002&apos;,master_log_pos=245; ##开启主从同步 MariaDB [mysql]&gt;start slave; ##查看主动同步状态 MariaDB [mysql]&gt;show slave status\G; ##主要看下面2项，都是YES表示成功了 Slave_IO_Running: Yes Slave_SQL_Running: Yes Master和Slave验证上面都没问题，那就开始测试了，去主服务断创建一个库，一张表，插入数据，再到从服务器来查看，如果数据一致表示没问题了 遇到的问题ERROR 2002 (HY000): Can&apos;t connect to local MySQL server through socket &apos;/tmp/mysql.sock&apos; (2) ####解决办法：在错误日志中，启动失败的原因极为明显，file ‘./mysql-bin。000004’ not found，failed to open！ mysql开启了bin日志功能，到数据库根目录查看该文件是存在的，可能是文件权限的问题。 [root@mysql-slave /]chown -R mysql:mysql /var/log/mysql [root@mysql-slave /]mysqld_safe &amp; ##启动安全模式 [root@mysql-slave /]systemctl stop mariadb.service [root@mysql-slave /]systemctl start mariadb.service ###再重新初始化数据库，就可以登录了 记得killall mysqld_safe mysql，因为不结束将不能开启二进制日志，结束安全模式进程然后 [root@mysql-slave /]systemctl stop mariadb.service [root@mysql-slave /]systemctl start mariadb.service]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>Mysql主从同步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 LVS集群+Keepalived（二）]]></title>
    <url>%2F2017%2F09%2F26%2FLVS%2BKeepalived%2F</url>
    <content type="text"><![CDATA[LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。本项目在1998年5月由章文嵩博士成立，是中国国内最早出现的自由软件项目之一。目前有三种IP负载均衡技术（NAT、DR、TUN），十种调度算法（rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq）Keepalived在这里主要用作RealServer的健康状态检查以及Master主机和BackUP主机之间failover的实现下面，搭建基于LVS+Keepalived的高可用负载均衡集群，其中，LVS实现负载均衡，但是，简单的LVS不能监控后端节点是否健康，它只是基于具体的调度算法对后端服务节点进行访问。同时，单一的LVS又存在单点故障的风险。在这里，引进了Keepalived，可以实现以下几个功能：1. 检测后端节点是否健康2. 实现LVS本身的高可用 目的：解决DR模型下Director单点故障 模型图： 准备工作:DR1: vip :192.168.1.150 eno16777736:192.168.1.130 DR2 : vip :192.168.1.150 eno16777736:192.168.1.131 RS1: eno16777736:192.168.1.132 lo:192.168.1.150 netmask 255.255.255.255 ##4个255表示将广播域限制在本机 RS2: eno16777736:192.168.1.133 lo:192.168.1.150 netmask 255.255.255.255 网络配置完成后都互相ping一下，看是不是都配通了 注意问题：1.清空iptables规则 iptables -t filter -F2.关闭selinux setenforce 0 安装httpdyum -y install httpd systemctl enable httpd.service 开启启动httpd服务 systemctl start httpd.service 启动服务 echo “WEB1”&gt;/var/www/html/index.html 在httpd下写一个 测试网页 curl http://localhost 测试一下看能不能看到“WEB1” 配置完后把VIP指向RS1 ifconfig lo 192.168.1.150 netmask 255.255.255.0 【DR2】配置同上 RS服务器上也要关闭防火墙，关闭selinux 安装ipvsyum -y install ipvsadm modprobe ip_vs #加载到内核 lsmod |grep ip_vs #查看是否有ipvs模块 【DR2】上同上配置 安装keepalivedyum -y install keepalived systemctl enable keepalived 开机启动 systemctl start keepalived 启动程序 yum -y install httpd ##这里装httpd做Server—server echo &lt;h1System upgrade maintenance &lt;\h1&gt;/var/www/html/index.html systemctl start httpd 修改配置:vim /etc/keepalived/keepalived.conf 配置文件如下： global_defs { notification_email { acassen@firewall.loc #设置报警邮件地址，可设置多个，每行一个 failover@firewall.loc #需要开启右键报警以及本机的sendmail服务 sysadmin@firewall.loc } notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 #设置SMTP Server地址 smtp_connect_timeout 30 router_id LVS_DEVEL } vrrp_instance VI_1 { state MASTER #指定keepalived的角色，MASTER为主服务器，BACKUP为备用服务器 interface eno16777736 virtual_router_id 50 priority 100 #定义优先级，数字越大，优先级越高，主Direvtor必须大于备用Director advert_int 1 authentication { auth_type PASS #设置验证类型 auth_pass 1111 #设置验证密码,最好随机生成几位数字 } virtual_ipaddress { 192.168.1.150 #设置主Director的VIP } virtual_server 192.168.1.150 80 { 设置VIP地址和端口 用空格隔开 delay_loop 6 #设置健康检查时间，单位为秒 lb_algo rr #设置负载均衡调度算法，默认为rr，即轮循算法，最优秀的是wlc算法 lb_kind DR #设置LVS实现LB机制，有NAT,DR,TUN三种模式 nat_mask 255.255.255.0 persistence_timeout 50 #回话保持时间，单位为秒 protocol TCP #指定转发协议类型，有TCP和UDP两种 real_server 192.168.1.132 80 { weight 2 #配置节点权值，数字越大权值越高 HTTP[_GET { url { path / digest ff20ad2481f97b1754ef3e12ecd3a9cc } url { path /mrtg/ digest 9b3a0c85a887a256d6939da88aabd8cd connect_timeout 3 #表示3秒无响应，则超时 nb_get_retry 3 #表示重试次数 delay_before_retry 3 #表示重试间隔 sorry_server 127.0.0.1 80 ##所有节点都down掉了， soory_server可以当real_server使用，或者定义升级维护页面 real_server 192.168.1.113 80 { weight 1 # HTTP_GET { # url { # path / # digest ff20ad2481f97b1754ef3e12ecd3a9cc # } #url { # path /mrtg/ #digest 9b3a0c85a887a256d6939da88aabd8cd # } connect_timeout 3 nb_get_retry 3 delay_before_retry 3 } } 【DR2】配置大致同上，只需修改部分参数 1. 把state MASTER 改为 state BACKUP 2. 把priority 100 的优先值改成90 然后保存即可 重启服务systemctl restart keepalived.service [DR1]#ipvsadm -L -n 出现以下信息表示配置成功，没有就返回上一步排错 IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 192.168.1.150:80 rr persistent 50 -192.168.1.113:80 Route 1 0 0 -192.168.1.132:80 Route 2 0 0 抑制ARP响应（防止将后端Real Server暴露在公网） 【DR1】# echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore 【DR1】# echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/ arp_announce 【DR1】# echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore 【DR1】# echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce arp响应限制: 1）arp_ignore: 0 - (默认值): 回应任何网络接口上对任何本地IP地址的arp查询请求 1 - 只回答目标IP地址是来访网络接口本地地址的ARP查询请求 2）arp_announce: 0 - (默认) 在任意网络接口（eth0,eth1，lo）上的任何本地地址 2 - 对查询目标使用最适当的本地地址.在此模式下将忽略这个IP数据包的源地址并尝试选择与能与该地址通信的本地地址. 首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址. 如果没有合适的地址被发现,将选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接口来进行发送. 测试DR1和DR2上把keepalived服务都启动起来systemctl keepalived status 查看服务当前状况在浏览器输入“192.168.1.150” 看能否看到“WEB1”和“WEB2”将DR2关掉看能否访问到“192.168.1.150”还能看到“WEB1”“WEB2”恭喜你 成功了！！！]]></content>
      <categories>
        <category>Cluster</category>
      </categories>
      <tags>
        <tag>Lvs+Keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 LB集群之LVS（一）]]></title>
    <url>%2F2017%2F09%2F26%2FLVS%2BNAT%2F</url>
    <content type="text"><![CDATA[LVS(linux virtue server)是一个虚拟的服务器集群系统。项目在1998年5月由章文嵩成立，是中国国内最早出现的自由软件项目之一。LVS集群采用IP负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序。为此，在设计时需要考虑系统的透明性、可伸缩性、高可用性和易管理性。 目的：利用LVS实现负载均衡 模型图： 配置IPlvs eth0： 192.168.1.103（VIP） eth1： 192.168.201.138（DIP）web1 eth0：192.168.201.137 lo：192.168.201.103web2 eth0：192.168.201.132 lo ：192.168.201.103 安装http服务[root@WEB1 ~]# yum -y install httpd[root@WEB1 ~]# systemctl enable http.service[root@WEB1 ~]# systemctl start http.service[root@WEB1 ~]# echo “1” &gt;/var/www/html/index.html[root@WEB1 ~]# curl http://localhostWEB2同上 安装ipvsadm[root@WEB1 ~]# yum -y install ipvsadm[root@WEB1 ~]# modprobe ip_vs #加载到内核[root@WEB1 ~]# ipvsadm #查看是否安装成功 配置IPVS[root@WEB1 ~]#ipvsadm -C 清空原来表格内容[root@WEB1 ~]# ipvsadm -A -t 192.168.1.103:80 -s rr #创建一个集群 -A添加地址 -t指定vip tcp端口 -s指定算法[root@WEB1 ~]# ipvsadm -a -t 192.168.1.103:80 -r 192.168.201.137:80 -m -a指定真实服务器 -t lvs上的vip -r 真实服务器[root@WEB1 ~]# ipvsadm -a -t 192.168.1.103:80 -r 192.168.201.132:80 -m -a指定真实服务器 -t lvs上的vip -r 真实服务器[root@WEB1 ~]# ipvsadm -L -n 查看IP Virtual Server version 1.2.1(size=4096)Prot LocalAddress:PortScheduler Flags-&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.1.103 wrr persistent 20-&gt; 192.168.201.137 Route 1 0 0-&gt; 192.168.201.132 Route 1 0 0 …表示成功了 配置RS给WEB1服务器配置VIPifconfig lo 192.168.1.103 netmask 255.255.255.255WEB2设置同上 验证浏览器输入VIP地址 192.168.1.103看WEB1和WEB2是不是轮询响应，down掉一个还能否响应]]></content>
      <categories>
        <category>Cluster</category>
      </categories>
      <tags>
        <tag>Lvs+NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 Jdk+Tomcat的安装]]></title>
    <url>%2F2017%2F09%2F26%2Ftomcat%2F</url>
    <content type="text"><![CDATA[Java Development Kit（JDK）是太阳微系统针对Java开发人员发布的免费软件开发工具包（SDK，Software development kit）。自从Java推出以来，JDK已经成为使用最广泛的Java SDK。由于JDK的一部分特性采用商业许可证，而非开源。因此，2006年太阳微系统宣布将发布基于GPL的开源JDK，使JDK成为自由软件。 目的：公司开发人员的需要 安装的版本：jdk版本：1.8.0_121tomcat版本：8.5.13 Jdk的安装：先查询是否安装JDK [root@node1 ~]# java -version #centos7 默认是装的openjdk Openjdk version &quot;1.8.0_65&quot; OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode) [root@node1 ~]# yum remove java-1.8.0-openjdk ##直接卸载 [root@node1 ~]# java -version #为什么还有在是吗？？？？ Openjdk version &quot;1.8.0_65&quot; OpenJDK Runtime Environment (build 1.8.0_65-b17) OpenJDK 64-Bit Server VM (build 25.65-b01, mixed mode) 正确的方式卸载openjdk： [root@node1 ~]# rpm -qa | grep java #查看装了那些包 java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64 java-1.7.0-openjdk-headless-1.7.0.91-2.6.2.3.el7.x86_64 java-1.8.0-openjdk-headless-1.8.0.65-3.b17.el7.x86_64 [root@node1 ~]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.91-2.6.2.3.el7.x86_64 ##三个都要卸载 [root@node1 ~]# java -version #OK，这次卸载的很干净 -bash: /bin/java: No such file or directory 下载你需要版本的jdk地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html [root@node1 ~]# rpm -ivh /usr/java/jdk1.7.0.79.rpm #安装JDK 配置环境变量在/etc/profile尾部添加以下代码： JAVA_PATH #标记是java的内容 export JAVA_HOME=/usr/java/jdk1.8.0_121 # export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar [root@node1 ~]# source /etc/profile #使配置生效 java -version #查看是否安装成功 TOMCAT的配置和安装：Tomcat是由Apache软件基金会下属的Jakarta项目开发的一个Servlet容器，按照Sun Microsystems提供的技术规范，实现了对Servlet和JavaServer Page（JSP）的支持，并提供了作为Web服务器的一些特有功能，如Tomcat管理和控制平台、安全域管理和Tomcat阀等。由于Tomcat本身也内含了一个HTTP服务器，它也可以被视作一个单独的Web服务器。但是，不能将Tomcat和Apache HTTP服务器混淆，Apache HTTP服务器是一个用C语言实现的HTTPWeb服务器；这两个HTTP web server不是捆绑在一起的。Apache Tomcat包含了一个配置管理工具，也可以通过编辑XML格式的配置文件来进行配置。 下载并安装TOMCAT[root@node1 ~]# tar -zxvf tomcat-8.5.13.tar.gz tomcat #解压安装文件并命名为&quot;tomcat&quot; [root@node1 ~]# mv tomcat /usr/local/ 启动tomcat[root@node1 ~]# bash catalina.sh start [root@node1 ~]# curl http://localhost:8080 #能看到tomcat的欢迎页面表示ok 修改TOMCAT默认端口[root@node1 ~]# vim /usr/local/tomcat/conf/server.xml **Connector port=&quot;**80**[S1] &quot; maxHttpHeaderSize=&quot;8192&quot;##默认端口是8080,看你需求修改 maxThreads=&quot;150&quot; minSpareThreads=&quot;25&quot; maxSpareThreads=&quot;75&quot; enableLookups=&quot;false&quot; redirectPort=&quot;8443&quot; acceptCount=&quot;100&quot; connectionTimeout=&quot;20000&quot; disableUploadTimeout=&quot;true&quot; /&gt;** [root@node1 bin]# ./shutdown.sh [root@node1 bin]# ./catalina.sh start #重新启动一下tomcat [root@node1 bin]# ss -tnlp #查看80、8005、8009是否为Java所监听 [root@node1 ~]#netstat -pant | grep 8005命令查看端口 [root@node1 ~]#fuser -v -n tcp 8005 #查看8005监听状态 -v 详细模式 -n 指定一个不同的命名空间 放行防火墙端口firewall-cmd --permanent --add-prot=80/tcp firewall-cmd --reload 开机启动[root@node1 ~]# vim /etc/rc.d/rc.local 添加下面三行内容： tomcat #标记是tomcat的内容 export JAVA_HOME=/usr/java/jdk1.8.0_121 # 是jdk安装目录 /usr/local/tomcat/bin/startup.sh start #是tomcat安装的目录 注意：修改rc.local文件添加执行权限： chmod +x rc.local 遇到的问题如果执行配置文件报错1： touch:cannot touc &apos;/usr/local/kencery/tomcat/logs/catalina.out:NO such file or directory *:报这个错只需要 mkdir /usr/local/tomcat/logs 即可 一般执行配置文件报错都会在/usr/local/tomcat/logs/catalina.out中显示 如果执行配置文件报错2： /usr/local/tomcat/bin/catalina.sh:line 435: /usr/local/java/jdk1.7.0_79/bin/java: no such file directory /usr/local/tomcat/bin/catalina.sh:line 435: /usr/local/jdk6/jre/bin/java: no such file or directory /usr/local/tomcat/bin/catalina.sh:line 435: /usr/local/jdk1.7.0_79//jre/bin/java: no such file or directory *:报这个错 只需要打开配置文件 /usr/local/tomcat/bin/catalina.sh 然后在435行用#注释掉即可]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 LAMP黄金组合之wordpress]]></title>
    <url>%2F2017%2F09%2F26%2Flamp%2Bweordpress%2F</url>
    <content type="text"><![CDATA[WordPress是一个注重美学、易用性和网络标准的个人信息发布平台。WordPress虽为免费的开源软件，但其价值无法用金钱来衡量。WordPress的图形设计在性能上易于操作、易于浏览；在外观上优雅大方、风格清新、色彩诱人。使用WordPress可以搭建功能强大的网络信息发布平台，但更多的是应用于个性化的博客。针对博客的应用，WordPress能让您省却对后台技术的担心，集中精力做好网站的内容。目的：LAMP搭建wordpress博客 准备工作： 1.上来先关闭防火墙 iptables -F 2.ping以下网络是否通畅 ping www.baidu.com 安装Httpd[root@wp ~]# yum -y install httpd [root@wp ~]# systemctl start httpd.service #启动httpd 防火墙放行80端口以及http服务 [root@wp ~]#firewell-cmd --permanent --add-service=http [root@wp ~]#fireewll-cmd --reload #重启防火墙生效 地址栏输入http://localhost 能看到apche测试页面即成功 安装mariadb[root@wp ~]# yum -y install mariadb mariadb-server [root@wp ~]# systemctl enabled mariadb.service #开机启动mysql [root@wp ~]# systemctl start mariadb.service #启动mysql 初始化mysql并设置密码： [root@wp ~]# mysql_secure_installation Enter current password for root (enter for none): #初次运行直接回车 Set root password? [Y/n] #是否设置root用户密码，输入y并回车或直接回车 New password: #设置root用户的密码 Re-enter new password: #再输入一次你设置的密码 Remove anonymous users? [Y/n] #是否删除匿名用户，回车 Disallow root login remotely? [Y/n] #是否禁止root远程登录 ,回车, Remove test database and access to it? [Y/n] # 是否删除test数据库,回车 Reload privilege tables now? [Y/n] #是否重新加载权限表，回车 始化MariaDB完成，接下来测试登录 root@wp ~]# mysql -u root -p #登录mysql 安装php[root@wp ~]# yum install php php-mysql php-gd libjpeg* php-ldap php-odbc php-pear php-xml php-xmlrpc php-mbstring php-bcmath php-mhash [root@wp ~]# vim /var/www/html/index.php &lt;?php phpinfo(); ?&gt; 保存退出！ systemctl restart httpd.service 浏览器地址栏输入http://localhost 能看到php测试页面即成功 配置wordpress进入wordpress目录 [root@wp ~]# cd /wordpress [root@wp ~]# cp wp-config-sample.php wp-config.php #复制主配置文并改名为wp-config.php 开始配置wp-config.php [root@wp ~]# vim wp-config.php / ** MySQL settings - You can get this info from your web host ** // /** The name of the database for WordPress */ define(&apos;DB_NAME&apos;, &apos;wordpressdb&apos;); #数据库名称 /** MySQL database username */ #数据库用户名 define(&apos;DB_USER&apos;, &apos;wordpressuser&apos;); /** MySQL database password */ #数据库密码 define(&apos;DB_PASSWORD&apos;, &apos;wordpresspassword&apos;); wq保存退出 配置完成后将wordpress目录下所有文件移动到/var/www/html下 chown -R apache:apache /var/www/html/ chmod -R 755 /var/www/html mysql授权配置[root@wp ~]# mysql -u root -p MariaDB [mysql]&gt; create database wordpressdb; #（创建库） MariaDB [mysql]&gt; create user wordpressuser@localhost identified by &apos;wordpresspassword&apos;; #（创建用户并设置密码） MariaDB [mysql]&gt; grant all privileges on wordpressdb.* to wordpressuser@localhost; #(给wordpressuser对wordpress授取所有权) MariaDB [mysql]&gt; flush privileges; #（重读数据库变量） 配置完重启服务: MariaDB [mysql]&gt; service httpd restart MariaDB [mysql]&gt; service mariadb restart #我一般不喜欢用Phpmyadmin，直接用Navicat 配置Wordpress浏览器地址栏输入http://localhost配置成功后应该是这个样子的，主题相关参数自行配置，是不是很时尚，wordpress有很多时尚的主题自己选择一个安装吧吧]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>LAMP+Wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 搭建ntp时间服务器]]></title>
    <url>%2F2017%2F09%2F25%2Fntp-server%2F</url>
    <content type="text"><![CDATA[目的：解决实验环境中时间的一致性NTP是Network Time Protocol的简写，即网络时间协议。多个主机可以通过NTP同步系统时间。下面我们搭建一个NTP服务器，其他服务器都可以通过NTP服务器实现时间同步。首先需要确保服务器时区设置是否正确，使用timedatectl查看设置时区（Asia/Shanghai）. 准备工作:NTP服务器地址： 服务器：192.168.1.50 客户端：192.168.1.51 RPM包检查：[root@ntp server ~]# rpm -ql |grep ntp 没有安装就直接yum安装 [root@ntp server ~]# yum -y install ntp 开机启动ntp服务systemctl enable ntpd systemctl start ntpd 获取免费时间服务器地址http://www.pool.ntp.org/zone/cn 配置时间服务器:[root@ntp server ~]# vim /etc/ntp.conf server 0.cn.pool.ntp.org server 1.cn.pool.ntp.org server 2.cn.pool.ntp.org server 3.cn.pool.ntp.org fudge 127.127.0.1 stratum 3 ##设置自身为3级Ntp server NTPserver server 127.127.1.0 iburst local clock ##外部不可用时，使用本地时间 restrict 192.168.1.0 mask 255.255.255.0 nomodify ## 设置客户端的限制，nomodify允许来自哪个段的IP来同步时间但不允许改ntp服务器参数 重启服务并防火墙放行systemctl restart ntpd firewall-cmd --permanent--add-port=123/udp firewall-cmd --reload 检查 ntpq -p查看ntpd客户端测试 [root@test ~]# ntpdate 192.168.1.50 29 Mar 15:13:01 ntpdate[3895]: adjust time server 192.168.1.50 offset -0.019419 sec ##同步时间成功 [root@test ~]# hwclock -w ##重新写入硬件时钟]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Ntp Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
